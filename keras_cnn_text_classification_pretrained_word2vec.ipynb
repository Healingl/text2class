{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过临时环境变量设置keras后端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "E:\\Anaconda2\\envs\\python35\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import re\n",
    "import codecs\n",
    "import pickle\n",
    "import fire\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import *  \n",
    "from keras.layers import Dense, Input, Concatenate ,Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding ,GlobalMaxPooling1D \n",
    "from keras.preprocessing.text import Tokenizer  \n",
    "from keras.preprocessing.sequence import pad_sequences  \n",
    "from keras.utils.np_utils import to_categorical  \n",
    "#from gensim.models.word2vec import Word2Vec\n",
    "#from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 语料库预处理  list(text)<->list(label_id)<->dict(label)\n",
    "- 文本列表   texts.append()\n",
    "- 文本标签id列表  labels.append(label_id)\n",
    "- 标签名：标签id 字典  labels_index[label_name] = label_id  \n",
    "- label_id = len(labels_index)\n",
    "\n",
    "```\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                f = open(fpath)\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本预处理   texts【每个文本转换为id序列，对齐最大文本长度】，label【每个标签转换为独热编码】\n",
    "\n",
    "```\n",
    "from keras.preprocessing.text import Tokenizer        #标记生成器\n",
    "from keras.preprocessing.sequence import pad_sequences   #填充序列\n",
    "\n",
    "tokenizer = Tokenizer()   #num_words：None或整数，处理的最大单词数量，只处理数据集词典中最常见的\n",
    "tokenizer.fit_on_texts(texts)  #在文本序列texts上执行标记生成器\n",
    "sequences = tokenizer.texts_to_sequences(texts)  #文本向量化，将文本转换为词id序列\n",
    "\n",
    "word_index = tokenizer.word_index   #词id（索引）\n",
    "print('Found %s unique tokens.' % len(word_index)) #所有唯一id\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)  #将每个文本的词id序列填充为最大长度\n",
    "\n",
    "labels = keras.utils.to_categorical(np.asarray(labels))  #将标签id转换为独热编码形式\n",
    "print('Shape of data tensor:', data.shape)  #文本列表的形状  length*MAX_SEQUENCE_LENGTH\n",
    "print('Shape of label tensor:', labels.shape)  #标签列表的形状  length*num_classes\n",
    "\n",
    "# split the data into a training set and a validation set #将数据分割为训练集和验证集\n",
    "indices = np.arange(data.shape[0])  #文本列表长度\n",
    "np.random.shuffle(indices)       #混排文本序号\n",
    "data = data[indices]           #混排后的文本列表\n",
    "labels = labels[indices]        #对应混排后的标签列表\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0]) #根据验证分割比例计算数据分割点\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取词向量字典  word->vector\n",
    "```\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载词向量字典  python3 使用KeyedVectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "wv = KeyedVectors.load_word2vec_format('./word_embedding.txt',binary=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(wv.vocab) len(wv.index2word) #词典大小"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wv['w23']#wv.word_vec('w23') #获取词的向量"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wv.index2word.index('w771340')  #词列表定位"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wv.index2word[407154]     #取对应索引的词"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wv[wv.index2word[0]]     # 取对应索引的向量"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wv.vector_size  # 向量维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立词向量矩阵"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "embedding_matrix = np.zeros((len(wv.index2word) + 1, wv.vector_size))  #词向量字典长度*维度\n",
    "for index,word in enumerate(wv.index2word):\n",
    "        embedding_matrix[index] = wv[word]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "embedding_matrix = np.random.uniform(-1,1,size=(len(word_index) + 1, EMBEDDING_DIM))    #语料库字典长度(+1)*维度  使用[-1,1]之间的均匀分布初始化\n",
    "for word, index in word_index.items():                                   #遍历语料库字典\n",
    "    if word in wv:                \n",
    "        embedding_matrix[index] = wv[word] #登录在词向量字典中的词覆盖初始化值"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(embedding_matrix)\n",
    "423693\n",
    "len(wv.index2word)\n",
    "423692"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载词向量矩阵到嵌入层"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(wv.index2word) + 1,#输入维度为词表大小+1，每个词使用独热编码\n",
    "                            wv.vector_size,#输出维度为词向量维度\n",
    "                            weights=[embedding_matrix],#权重=[词向量矩阵]\n",
    "                            input_length=100,#输入长度为最大序列长度\n",
    "                            trainable=True)#是否更新参数"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "embedding_matrix = np.random.random((10000,256))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(10000,#输入维度为词表大小+1，每个词使用独热编码\n",
    "                            256,#输出维度为词向量维度\n",
    "                            weights=[embedding_matrix],#权重=[词向量矩阵]\n",
    "                            input_length=100,#输入长度为最大序列长度\n",
    "                            trainable=True)#是否更新参数\n",
    "# 输入1D张量(最大文本长度) 输出2D张量(最大文本长度*维度)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "shape形状 ： batch批 个 length长 dim维"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "embedding_layer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用cnn实现文本分类 序列模型\n",
    "```\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Conv1D,MaxPooling1D,GlobalMaxPooling1D,Dropout,Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000,256,weights=[embedding_matrix],input_length=100,trainable=True))\n",
    "model.add((100,[3,4,5],strides=[1,1,1],activation='relu'))     #filters:100个卷积核，kernel_size:多个卷积窗口大小\n",
    "#model.add(MaxPooling1d(pool_size=[100-3+1,100-4+1,100-5+1],strides=[1,1,1]) \n",
    "model.add(GlobalMaxPool1D())                  #窗口为样本长度（最大文本长度）-卷积窗口大小+1，步长为1的最大池化层\n",
    "model.add(Dropout(0.5))                          #防止过拟合，随机丢弃50%的连接\n",
    "model.add(Dense(len(labels_index),activation='softmax')\n",
    "\n",
    "model.compile('rmsprop','categorical_crossentropy',['acc'])\n",
    "\n",
    "model.fit(x_train,y_train,validation_data=(x_val,y_val),epochs=10,batch_size=128)\n",
    "\n",
    "#score = model.evaluate(x_test,y_test,batch_size=128)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用cnn实现文本分类 函数式模型\n",
    "```\n",
    "sequence_input = keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')  #后端张量+某些属性=keras张量\n",
    "embedded_sequences = embedding_layer(sequence_input)     #给层赋值\n",
    "x = Conv1D(100, 5, activation='relu')(embedded_sequences) #一维卷积层（时序卷积），位置参数，卷积个数，卷积核大小\n",
    "x = MaxPooling1D(5)(x) #一维池化层，最大池化窗口\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)         #函数式模型`model = Model(input=[a, b], output=c)`，通过两端的输入输出初始化模型\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=2, batch_size=128)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型可视化\n",
    "```\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "embedding_matrix = np.random.random((10000,256))\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Conv1D,MaxPooling1D,GlobalMaxPooling1D,Dropout,Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000,256,weights=[embedding_matrix],input_length=100,trainable=True))\n",
    "model.add(Conv1D(100,3,strides=1,activation='relu')) \n",
    "model.add(GlobalMaxPooling1D())                \n",
    "model.add(Dropout(0.5))                    \n",
    "model.add(Dense(10,activation='softmax'))\n",
    "          \n",
    "model.compile('rmsprop','categorical_crossentropy',['acc'])\n",
    "model.summary()\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_10 (Embedding)        (None, 100, 256)            2560000 【100个256维】\n",
    "_________________________________________________________________\n",
    "conv1d_10 (Conv1D)      (None, 98, 100) 【100种卷积*98(100-3+1)个特征】 76900【100种*(3长*256宽+1偏置)】     \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_2 (Glob     (None, 100)  【100*(max(98->1))】   0         \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)           (None, 100)                0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 10)                1010  ？【10类*(100个+1偏置)】\n",
    "=================================================================\n",
    "Total params: 2,637,910\n",
    "Trainable params: 2,637,910\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextCNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer  \n",
    "from keras.preprocessing.sequence import pad_sequences  \n",
    "from keras.utils.np_utils import to_categorical   \n",
    "from keras.layers import Dense, Input, Concatenate ,Flatten, Conv1D, MaxPooling1D, Embedding ,GlobalMaxPooling1D \n",
    "from keras.models import Sequential, Model  \n",
    "from keras.optimizers import *  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    '''\n",
    "    A simple TextCNN inplemetation in keras\n",
    "    '''\n",
    "    def __init__(self,embedding_matrix,input_length,num_classes,\n",
    "                 embedding_pretrained=False,vocab_size=None,embedding_dim=None,trainable=True,\n",
    "                 num_filters=100,filter_sizes=[3,4,5],\n",
    "                 dropout_rate=0.5,optimizer='Adadelta',binary=False,\n",
    "                ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.embedding_pretrained = embedding_pretrained\n",
    "        self.input_length = input_length\n",
    "        self.trainable = trainable\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.binary = binary\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def complie(self):\n",
    "        # Input-Embedding\n",
    "        if self.embedding_pretrained:\n",
    "            embedding_layer = Embedding(self.vocab_size,\n",
    "                            self.embedding_dim,\n",
    "                            input_length=self.input_length)\n",
    "        else:\n",
    "            embedding_layer = Embedding(self.embedding_matrix.shape[0],\n",
    "                            self.embedding_matrix.shape[1],\n",
    "                            weights=[self.embedding_matrix],\n",
    "                            input_length=self.input_length,\n",
    "                            trainable=self.trainable)\n",
    "        Input_ = Input(shape=(self.input_length,), dtype='int32')\n",
    "        embedding_layer_ = embedding_layer(Input_)\n",
    "        # Conv-Pool\n",
    "        Conv = [0]*len(self.filter_sizes)\n",
    "        GMP = [0]*len(self.filter_sizes)\n",
    "        if type(self.filter_sizes) == list:\n",
    "            for index,filter_size in enumerate(self.filter_sizes):\n",
    "                Conv[index] = Conv1D(self.num_filters,filter_size,activation='relu')(embedding_layer_)\n",
    "                GMP[index] = GlobalMaxPooling1D()(Conv[index])\n",
    "        else:\n",
    "            Conv[0] = Conv1D(self.num_filters,self.filter_sizes,activation='relu')(embedding_layer_)\n",
    "            GMP[0] = GlobalMaxPooling1D()(Conv[0])\n",
    "        # Concatenate-Flatten-Dropout-Dense\n",
    "        Concatenate_ = Concatenate()([gmp for gmp in GMP])\n",
    "        #Flatten_ = Flatten()(Concatenate_) for2D->1D\n",
    "        Dropout_ = Dropout(self.dropout_rate)(Concatenate_)\n",
    "        if self.binary == True:\n",
    "            labels = Dense(1, activation='sigmoid')(Dropout_)\n",
    "\n",
    "            self.model = Model(inputs=Input_,outputs=labels)\n",
    "            self.model.compile(loss='binary_crossentropy', \n",
    "                          optimizer=self.optimizer,\n",
    "                          metrics=['acc'])\n",
    "        else:\n",
    "            labels = Dense(self.num_classes,activation='softmax')(Dropout_)\n",
    "            self.model = Model(inputs=Input_,outputs=labels)\n",
    "            self.model.compile(loss='categorical_crossentropy', \n",
    "                          optimizer=self.optimizer,\n",
    "                          metrics=['acc'])  \n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "        \n",
    "    def fit(self,x,y,epochs=1,batch_size=128,validation_split=0.2):\n",
    "        self.model.fit(x,y,validation_split=validation_split,\n",
    "                       epochs=epochs, batch_size=batch_size)\n",
    "        \n",
    "    def evaluate(self,x_test,y_test,batch_size=128):\n",
    "        #score = self.model.evaluate(x_test, y_test, batch_size)\n",
    "        pass\n",
    "        \n",
    "    def save(self):\n",
    "        pass\n",
    "    \n",
    "    def load(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tc = TextCNN(embedding_matrix,100,10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "tc.complie()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tc.summary()\n",
    "Layer (type)                     Output Shape          Param #     Connected to                     \n",
    "====================================================================================================\n",
    "input_13 (InputLayer)            (None, 100)           0                                            \n",
    "____________________________________________________________________________________________________\n",
    "embedding_23 (Embedding)         (None, 100, 256)      2560000     input_13[0][0]                   \n",
    "____________________________________________________________________________________________________\n",
    "conv1d_39 (Conv1D)               (None, 98, 100)       76900       embedding_23[0][0]               \n",
    "____________________________________________________________________________________________________\n",
    "conv1d_40 (Conv1D)               (None, 97, 100)       102500      embedding_23[0][0]               \n",
    "____________________________________________________________________________________________________\n",
    "conv1d_41 (Conv1D)               (None, 96, 100)       128100      embedding_23[0][0]               \n",
    "____________________________________________________________________________________________________\n",
    "global_max_pooling1d_27 (GlobalM (None, 100)           0           conv1d_39[0][0]                  \n",
    "____________________________________________________________________________________________________\n",
    "global_max_pooling1d_28 (GlobalM (None, 100)           0           conv1d_40[0][0]                  \n",
    "____________________________________________________________________________________________________\n",
    "global_max_pooling1d_29 (GlobalM (None, 100)           0           conv1d_41[0][0]                  \n",
    "____________________________________________________________________________________________________\n",
    "concatenate_8 (Concatenate)      (None, 300)           0           global_max_pooling1d_27[0][0]    \n",
    "                                                                   global_max_pooling1d_28[0][0]    \n",
    "                                                                   global_max_pooling1d_29[0][0]    \n",
    "____________________________________________________________________________________________________\n",
    "dropout_7 (Dropout)              (None, 300)           0           concatenate_8[0][0]              \n",
    "____________________________________________________________________________________________________\n",
    "dense_6 (Dense)                  (None, 10)            3010        dropout_7[0][0]                  \n",
    "====================================================================================================\n",
    "Total params: 2,870,510\n",
    "Trainable params: 2,870,510\n",
    "Non-trainable params: 0\n",
    "____________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "corpus_path =  'train-zhihu6-title-desc-single-100000.txt'\n",
    "def corpus_preprocess(corpus_path,label_re):\n",
    "    label_pattern = label_re+'[\\w]+'\n",
    "    corpus_path =  os.path.abspath(corpus_path)\n",
    "    corpus_size = os.path.getsize(corpus_path)/(1024*1024*1024)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    label_index = {}\n",
    "    with codecs.open(corpus_path,'r',encoding='utf-8') as f:\n",
    "        start = time.time()\n",
    "        if corpus_size > 0:\n",
    "            while True:\n",
    "                try:\n",
    "                    line = f.readline()\n",
    "                    line = line.strip()\n",
    "                    re_labels = re.findall(label_pattern,line)\n",
    "                    if re_labels != None and len(re_labels) > 0:\n",
    "                        texts.append(re.sub(label_pattern,'',line))\n",
    "                        for i in re_labels:\n",
    "                            if i not in label_index:\n",
    "                                label_id = len(label_index)\n",
    "                                label_index[i] = label_id\n",
    "                                labels.append(label_id)\n",
    "                            else:\n",
    "                                labels.append(label_index[i])\n",
    "                except:\n",
    "                    print(time.time()-start)\n",
    "                    break       \n",
    "        else:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                re_labels = re.findall(label_pattern,line)\n",
    "                if re_labels != None and len(re_labels) > 0:\n",
    "                    texts.append(re.sub(label_pattern,'',line))\n",
    "                    for i in re_labels:\n",
    "                        if i not in label_index:\n",
    "                            label_id = len(label_index)\n",
    "                            label_index[i] = label_id\n",
    "                            labels.append(label_id)\n",
    "                        else:\n",
    "                            labels.append(label_index[i])\n",
    "            print(time.time()-start)\n",
    "                \n",
    "        f.close()\n",
    "    return texts,labels,label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2060.216057777405\n"
     ]
    }
   ],
   "source": [
    "texts,labels,label_index = corpus_preprocess(corpus_path,'__label__')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_preprocess"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def text_preprocess(texts,labels):\n",
    "    MAX_SEQUENCE_LENGTH = max([len(text.split()) for text in texts])\n",
    "    print(MAX_SEQUENCE_LENGTH)\n",
    "    tokenizer = Tokenizer()  \n",
    "    tokenizer.fit_on_texts(texts)  \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    print(sequences[:2])\n",
    "    word_index = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) \n",
    "    labels = to_categorical(np.asarray(labels)) \n",
    "    return data,labels,word_index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "max([len(text.split()) for text in texts])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "data,labels,word_index  = text_preprocess(texts,labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "word2vec_path = './word_embedding.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec_preprocess"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def word2vec_preprocess(word2vec_path,word_index,embedding=256):\n",
    "    word2vec_path = os.path.abspath(word2vec_path)\n",
    "    wv = KeyedVectors.load_word2vec_format(word2vec_path,binary=False)\n",
    "    embedding_matrix = np.random.uniform(-1,1,size=(len(word_index) + 1, embedding))\n",
    "    for word, index in word_index.items():                                 \n",
    "        if word in wv:                \n",
    "            embedding_matrix[index] = wv[word]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "embedding_matrix  = word2vec_preprocess(word2vec_path,word_index,embedding=256)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "temp = {'embedding_matrix':embedding_matrix}\n",
    "with codecs.open('./test.pkl','wb') as f:\n",
    "    pickle.dump(temp,f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "tc = TextCNN(embedding_matrix,120,971)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tc.complie()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "tc.fit(data,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import gzip\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import gzip\n",
    "import codecs\n",
    "import _pickle as pickle\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences  \n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def _to_categorical(num,max_num):\n",
    "    arr = np.zeros((1,max_num),dtype=np.int8)\n",
    "    arr[0][num] = 1\n",
    "    return arr[0]\n",
    "\n",
    "def index2categorical(labels,max_num):\n",
    "    labels = list(np.array(labels).flatten())\n",
    "    num_labels = len(labels)\n",
    "    categorical = []\n",
    "    for num in range(max_num):\n",
    "        arr = np.zeros((1,max_num),dtype=np.float16) #np.int8\n",
    "        arr[0][num] = labels.count(num)/num_labels  #prior probability for each label \n",
    "        # uniform distribution?\n",
    "        categorical.append(arr[0])\n",
    "    return categorical\n",
    "\n",
    "def normalize(distribution):\n",
    "    print(distribution)\n",
    "    total = sum(distribution)\n",
    "    return [p/total for p in distribution]\n",
    "\n",
    "class Corpus(object):\n",
    "    '''\n",
    "    Build train/dev/test data easily and quickly!\n",
    "    '''\n",
    "    def __init__(self,path,word2vec_path=None,label_pattern='__label__[\\-\\w]+'):\n",
    "        self.path = os.path.abspath(path)\n",
    "        self.filename = os.path.basename(self.path).split('.')[0] \n",
    "        self.label_pattern = label_pattern\n",
    "        self.size = round(os.path.getsize(path)/(1024*1024*1024),2)\n",
    "        self.texts = []\n",
    "        self.max_text_length = 0\n",
    "        self.labels = []\n",
    "        self.word_index = {'__PADDING__':0}\n",
    "        self.label_index = {}\n",
    "        self.word2vec_path = word2vec_path\n",
    "\n",
    "    def preprocess(self):\n",
    "        start = time.time()\n",
    "        with codecs.open(self.path,'r',encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip()\n",
    "                re_labels = re.findall(self.label_pattern,line)\n",
    "                text = re.sub(self.label_pattern,'',line)\n",
    "                # if each line with multilabels\n",
    "                if re_labels != None and len(re_labels) > 0:# for multilabel\n",
    "                    word_ids = []\n",
    "                    for word in text.split(' '):#text preprocess\n",
    "                        if word not in self.word_index:\n",
    "                            word_id = len(self.word_index)\n",
    "                            self.word_index[word] = word_id\n",
    "                            word_ids.append(word_id)\n",
    "                        else:\n",
    "                            word_ids.append(self.word_index[word])\n",
    "                    word_ids_length = len(word_ids)\n",
    "                    if word_ids_length > self.max_text_length:\n",
    "                        self.max_text_length = word_ids_length\n",
    "                    self.texts.append(word_ids)\n",
    "                    label_ids = []\n",
    "                    for label in re_labels:\n",
    "                        if label not in self.label_index:\n",
    "                            label_id = len(self.label_index)\n",
    "                            self.label_index[label] = label_id\n",
    "                            label_ids.append(label_id)\n",
    "                        else:\n",
    "                            label_ids.append(self.label_index[label])\n",
    "                    self.labels.append(label_ids)\n",
    "        self.num_words = len(self.word_index)\n",
    "        self.texts = np.array(pad_sequences(self.texts,\n",
    "                                   maxlen=self.max_text_length,\n",
    "                                   padding='post',\n",
    "                                   truncating='post',\n",
    "                                   value=0),dtype=np.int32)\n",
    "        self.num_texts = len(self.texts)\n",
    "        self.num_classes = len(self.label_index)\n",
    "        #self.index_label = dict(zip(self.label_index.values(),self.label_index.keys()))#index2label\n",
    "        #self.labels = np.array([to_categorical(label,self.num_classes)[0] for label in self.labels])\n",
    "        categorical = index2categorical(self.labels,self.num_classes)\n",
    "        for index,label_ids in enumerate(self.labels):\n",
    "            self.labels[index] = sum([categorical[label_id] for label_id in label_ids])\n",
    "        self.labels = np.array(self.labels)\n",
    "        print(self.labels[:10])\n",
    "        self.num_labels = len(self.labels)\n",
    "        assert self.num_texts == self.num_labels\n",
    "        # preprocess pretrained word2vec\n",
    "        if not self.word2vec_path == None: \n",
    "            self.embeddings_index = {}\n",
    "            vectors = 0\n",
    "            with codecs.open(self.word2vec_path,'r',encoding='utf-8') as f:\n",
    "                f.readline()\n",
    "                while True:\n",
    "                    try:\n",
    "                        line = f.readline()\n",
    "                        values = line.split()\n",
    "                        word = values[0]\n",
    "                        vectors = np.asarray(values[1:], dtype='float16')#float32\n",
    "                        self.embeddings_index[word] = vectors\n",
    "                        f.close()\n",
    "                    except:\n",
    "                        break\n",
    "            self.vector_dim = len(vectors)\n",
    "            self.embedding_matrix = np.zeros((self.num_words + 1,self.vector_dim))\n",
    "            for word, index in self.word_index.items():                                 \n",
    "                if word in self.embeddings_index:                \n",
    "                    self.embedding_matrix[index] = self.embeddings_index[word]\n",
    "                else:\n",
    "                    self.embedding_matrix[index] = np.random.uniform(-1,1,size=(self.vector_dim)) #unlogin word  \n",
    "            self.num_embeddings = len(self.embeddings_index)\n",
    "            self.embedding_matrix_shape =self.embedding_matrix.shape\n",
    "        else:\n",
    "            self.embedding_matrix = None\n",
    "        self.preprocess_time = round(time.time() - start,2)\n",
    "\n",
    "    def summary(self):\n",
    "        print('path:',self.path,\n",
    "              '\\nfilename:',self.filename,\n",
    "              '\\nlabel_pattern:',self.label_pattern,\n",
    "              '\\nsize: %sGB'%self.size,\n",
    "              '\\nnum_texts:',self.num_texts,\n",
    "              '\\ntexts_shape:',self.texts.shape,\n",
    "              '\\nnum_labels:',self.num_labels,\n",
    "              '\\nlabels_shape:',self.labels.shape,\n",
    "              '\\nnum_words:',self.num_words,\n",
    "              '\\nnum_classes:',self.num_classes,\n",
    "              '\\nmax_text_length:',self.max_text_length,\n",
    "              '\\npreprocess_time: %ss'%self.preprocess_time\n",
    "             )\n",
    "        if not self.word2vec_path == None:\n",
    "            print('num_embeddings:',self.num_embeddings,\n",
    "                  '\\nvector_dim:',self.vector_dim,\n",
    "                  '\\nembedding_matrix_shape:',self.embedding_matrix_shape\n",
    "             )\n",
    "\n",
    "    @staticmethod\n",
    "    def dump(corpus):\n",
    "        corpus_object_path = os.path.join(os.path.dirname(corpus.path),\n",
    "                            corpus.filename+'.'+corpus.__class__.__name__+'.pkl.gz')\n",
    "        with gzip.open(corpus_object_path,'wb') as f:\n",
    "            pickle.dump(corpus,f)\n",
    "            print(corpus_object_path,\n",
    "                ': %sGB'%round(os.path.getsize(corpus_object_path)/(1024*1024*1024),2))\n",
    "\n",
    "    @staticmethod\n",
    "    def load(corpus_path):\n",
    "        corpus_path = os.path.abspath(corpus_path)\n",
    "        with gzip.open(corpus_path,'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    @classmethod\n",
    "    def transform(cls,corpus):\n",
    "        corpus.preprocess()\n",
    "        corpus.summary()\n",
    "        #cls.dump(corpus)\n",
    "\n",
    "def main():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Corpus('fsd_gs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.58251953  0.          0.          0.        ]\n",
      " [ 0.58251953  0.          0.          0.        ]\n",
      " [ 0.58251953  0.          0.          0.        ]\n",
      " [ 0.58251953  0.          0.          0.        ]\n",
      " [ 0.58251953  0.          0.          0.        ]\n",
      " [ 0.58251953  0.          0.          0.        ]\n",
      " [ 0.58251953  0.          0.          0.        ]\n",
      " [ 0.          0.26611328  0.          0.        ]\n",
      " [ 0.          0.          0.11309814  0.        ]\n",
      " [ 0.58251953  0.          0.          0.        ]]\n",
      "path: C:\\Users\\lxp\\Desktop\\nlp\\quora\\kaggle\\zhihu\\ieee_zhihu_cup\\fsd_gs.txt \n",
      "filename: fsd_gs \n",
      "label_pattern: __label__[\\-\\w]+ \n",
      "size: 0.0GB \n",
      "num_texts: 3033 \n",
      "texts_shape: (3033, 108) \n",
      "num_labels: 3033 \n",
      "labels_shape: (3033, 4) \n",
      "num_words: 12139 \n",
      "num_classes: 4 \n",
      "max_text_length: 108 \n",
      "preprocess_time: 0.13s\n"
     ]
    }
   ],
   "source": [
    "Corpus.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [[0,1],[2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(sum([j+1 for j in i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus.labels.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "to_categorical(5,10)\n",
    "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pad_sequences([[1,2,3],[4,5,6]],12)\n",
    "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 6]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(range(1000000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Corpus('fsd_gs.txt',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: C:\\Users\\lxp\\Desktop\\nlp\\quora\\kaggle\\zhihu\\ieee_zhihu_cup\\fsd_gs.txt \n",
      "filename: fsd_gs \n",
      "label_pattern: __label__[\\-\\w]+ \n",
      "size: 0.0GB \n",
      "num_texts: 3033 \n",
      "num_labels: 3033 \n",
      "num_words: 12139 \n",
      "num_classes: 4 \n",
      "max_text_length: 108 \n",
      "preprocess_time: 0.16s\n"
     ]
    }
   ],
   "source": [
    "corpus.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lxp\\Desktop\\nlp\\quora\\kaggle\\zhihu\\ieee_zhihu_cup\\fsd_gs.Corpus.pkl.gz : 0.0GB\n"
     ]
    }
   ],
   "source": [
    "corpus.dump(corpus)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpus_ = corpus.load('fsd_gs_corpus.pkl.gz')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpus_.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度学习模型基类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    '''\n",
    "    A simple Neural Network inplemetation in keras\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.name = self.__class__.__name__\n",
    "        \n",
    "    def build(self):\n",
    "        pass\n",
    "        \n",
    "    def compile(self):\n",
    "        self.model.compile(loss=self.loss, \n",
    "                          optimizer=self.optimizer,\n",
    "                          metrics=self.metrics)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def plot_model(self):\n",
    "        plot_model(self.model,\n",
    "                to_file=self.name+'.png',\n",
    "                show_shapes=True,\n",
    "                show_layer_names=True)\n",
    "\n",
    "    def fit(self,x,y,epochs=5,batch_size=128,validation_split=0.1):\n",
    "        self.model.fit(x,y,validation_split=validation_split,\n",
    "                       epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    def evaluate(self,x_test,y_test,batch_size=128):\n",
    "        score = self.model.evaluate(x_test, y_test, batch_size)\n",
    "        \n",
    "    @staticmethod\n",
    "    def save_to_yaml(model):\n",
    "        yaml_string = model.model.to_yaml()\n",
    "        with gzip.open(model.name+'.config.yml.gz','wb') as f:\n",
    "            f.write(yaml_string.encode('utf-8'))\n",
    "        model.model.save_weights(model.name+'.weights.h5')\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_from_yaml(config_path,weights_path):\n",
    "        with gzip.open(config_path,'rb') as f:\n",
    "            yaml_string = f.read()\n",
    "        model = model_from_yaml(yaml_string.decode('utf-8'))\n",
    "        model.load_weights(weights_path, by_name=True)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def dump_to_pickle(model):\n",
    "        with gzip.open(model.name+'.pkl.gz','wb') as f:\n",
    "            pickle.dump(model,f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_pickle(model_pickle_path):\n",
    "        model_pickle_path = os.path.abspath(model_pickle_path)\n",
    "        with gzip.open(model_pickle_path,'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    @classmethod   \n",
    "    def train(model):\n",
    "        model.build()\n",
    "        model.compile()\n",
    "        model.summary()\n",
    "        model.fit()\n",
    "        NN.save_to_yaml(model)\n",
    "        NN.dump_to_pickle(model)\n",
    "        \n",
    "    @staticmethod\n",
    "    def predict(self):\n",
    "        pass\n",
    "#if __name__ == '__main__':\n",
    " #   fire.Fire(NN.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextRNN(NN):\n",
    "    def __init__(self,**kw):\n",
    "        print(kw)\n",
    "        self.a = kw\n",
    "        del kw\n",
    "        pass\n",
    "    def train(num_filters=100, filter_sizes=[1,2,3],):\n",
    "        pass\n",
    "    @classmethod\n",
    "    def test(cls):\n",
    "        print(type(cls.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = {'a':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1}\n"
     ]
    }
   ],
   "source": [
    "tr = TextRNN(**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.update({'b':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1}"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "TextRNN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NN.train of <class '__main__.NN'>>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'build',\n",
       " 'compile',\n",
       " 'dump_to_pickle',\n",
       " 'evaluate',\n",
       " 'fit',\n",
       " 'load_from_pickle',\n",
       " 'load_from_yaml',\n",
       " 'plot_model',\n",
       " 'predict',\n",
       " 'save_to_yaml',\n",
       " 'summary',\n",
       " 'train']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
